\documentclass[3p,times]{elsarticle}
\usepackage{setspace}
\usepackage{algcompatible}
\usepackage[usenames,dvipsnames]{color}
\usepackage{amsthm,amsmath}
\newtheorem{definition}{Definition}
\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{hyperref}% add hypertext capabilities
\usepackage[mathlines]{lineno}
\usepackage{amssymb}
\usepackage[figuresright]{rotating}


\usepackage{Sweave}
\begin{document}
\input{Paper-concordance}
\input{title}
\input{abstract}

\section{Introduction}
For the time being, I am studying the statistical properties of the Latent Dirichlet Allocation topic model.

\section{Background on LDA}
\noindent With an increase in the amount of digitized text, topic modeling has become an important area of research. One of the most popular methods for topic modeling is Latent Dirichlet Allocation (LDA) first described in \cite{lda}. LDA is a probabilistic hierarchical generative model, which uses the Bayesian framework to discover hidden structure, or \textit{topics},  within a text Corpus. \textbf{For a more thorough background in topic modeling, refer to \cite{news}.}

\subsection{Generative Process}
\noindent Given a set of $K$ topics $k=1,...K$ and a set of $D$ documents $d=1,...,D$, also referred to as a corpus, the vocabulary is the set of unique words in the Corpus say $w_{1,1},...,w_{d,n_{d}},...,w_{D,n_{D}}$, where $n_{d}$ is the number of uniques words in document $d$ and $V=\sum_{d=1}^{D}{n_{d}}$. The goal of this algorithm as stated by Blei, ``to find a probabilistic model of a corpus that not only assigns high probability to members of the corpus, but also assigns high probability to other `similar' documents" (\cite{lda}, pp.996). The generative process for a corpus is as follows:
\begin{itemize}
	\item For each topic, \\
	1. Draw $\beta^{k} \sim Dir(\eta)$, a distribution for each topic over the vocabulary.
	
	\item For each document, \\
	2. Draw $\theta^{d} \sim Dir(\alpha)$, a distribution for each document over the topics.	
		\begin{itemize}
		\item For each word in document $d$, \\
		3. Draw $z_{d,n} \sim Mult(\theta^{d})$, a topic assignment (i.e. $z_{d,n} \in \mathbb{Z} \in \{1,...,K\}$ ). \\
		4. Draw $w_{d,n} \sim Mult(\beta^{z_{d,n}})$, the $n$th word in document $d$.
	\end{itemize}
\end{itemize}

\noindent This process requires that we know the number of topics $K$ a priori. Blei also assumed that the hyperparameters $\eta$ and $\alpha$ are fixed and can be estimated through the given data. Assuming this model, we can define the joint probability distribution of a corpus as:
\begin{equation} \label{eq1}
p(\beta,\theta,z_{d,n},w_{d,n} | \alpha,\eta) = \prod_{k=1}^{K}{p(\beta^{k} | \eta)}  \prod_{d=1}^{D}\left( p(\theta^{d} | \alpha) \prod_{n=1}^{n_{d}}{p(z_{d,n} | \theta^{d})}p(w_{d,n} | \beta^{1},...,\beta^{K},z_{d,n}) \right)  ,
\end{equation}

\noindent We will use the following dot notation:

\begin{align*}
c_{k,d,v}&=\sum_{n=1}^{n_{d}}{I(z_{d,n}=k \& w_{d,n}=v)}; &\text{ \# of times word $v$ is assigned to topic $k$ in document $d$} \\
c_{k,.,v}&=\sum_{d=1}^{D}{c_{k,d,v}}; &\text{ \# of words in document $d$ assigned to topic $k$} \\
c_{k,d,.}&=\sum_{v=1}^{V}{c_{k,d,v}}; &\text{ \# of times word $v$ is assigned to topic $k$ in any document} \\
c_{k,.,.}&=\sum_{d=1}^{D}\sum_{v=1}^{V}{c_{k,d,v}}; &\text{ \# of words in corpus assigned to topic $k$} 
\end{align*}


\noindent Let $\Gamma(\cdot)$ be the Gamma function, then:

\begin{align*}
p(\beta^{k} | \eta) &=\frac{ \Gamma( \sum_{v=1}^{V}{ \eta_{v} } )}{ \prod_{v=1}^{V}{\Gamma(\eta_{v})}}  
\prod_{v=1}^{V}{(\beta^{k}_{v})^{\eta_{v}-1}} \\
p(\theta^{d} | \alpha) &=\frac {\Gamma( \sum_{k=1}^{K}{ \alpha_{k} } )}{ \prod_{k=1}^{K}{\Gamma(\alpha_{k})}}  
\prod_{k=1}^{K}{(\theta^{d}_{k})^{\alpha_{k}-1}} \\
p(z_{d,n} | \theta) &=\frac{1}{\prod_{k=1}^{K}{c_{d,k,.}!}}
\prod_{k=1}^{K}{(\theta^{d}_{k})^{c_{d,k,.}}} \\
p(w_{d,n} | \beta,z) &=\frac{1}{\prod_{v=1}^{V}{c_{k,.,v}!}}\prod_{v=1}^{V}{
(\beta^{z_{d,n}}_{v})^{c_{k,.,v}}}
\end{align*}

\subsection{Posterior Inferance}
\noindent In practice, we do not know either $\beta$ or $\theta$. All we are given is a set of documents providing the $w_{d,n}$ parameters. Most often, we are interested in the latent variables $\beta$ and $\theta$. Rewriting \ref{eq1}

\begin{equation} \label{eq2}
p(w, z, \theta, \beta | \alpha,\eta)=\left( \prod_{k=1}^{K} \frac{\Gamma(\sum{\eta_{v}})} {\prod_{v=1}^{V}(\Gamma(\eta_{v}))} \prod_{v=1}^{V} (\beta^{k}_{v})^{(\eta_{v}-1+c_{k,.,v})} \right) * 
\left( \prod_{d=1}^{D} \frac{\Gamma(\sum{\alpha_{k}})} {\prod_{k=1}^{K}(\Gamma(\alpha_{k}))} \prod_{k=1}^{K} (\theta^{d}_{k})^{(\alpha_{k}-1+c_{k,d,.})} \right)
\end{equation}

\noindent Integrating out the latent variables, we obtain the following posterior distribution of a corpus:

\begin{equation} \label{eq3}
p(w| \theta, \beta, z)=\int_{\theta} \int_{\beta} \sum_{z} \left( \prod_{k=1}^{K}{\frac{\Gamma(\sum{\eta_{v}})}{\prod_{v=1}^{V}(\Gamma(\eta_{v}))} \prod_{v=1}^{V}{(\beta^{k}_{v})^{\eta_{v}-1+c_{k,.,v}} } }    \right) * \left( \prod_{d=1}^{D}{\frac{\Gamma(\sum{\alpha_{k}})}{\prod_{k=1}^{K}(\Gamma(\alpha_{k}))}\prod_{k=1}^{K}{
	(\theta^{d}_{k})^{\alpha_{k}-1+c_{k,d,.}} } }    \right) d\beta d\theta
\end{equation}

\noindent This distribution can be used for maximum likelihood estimation and to find the distribution of the latent variables. However, it has been proven to be intractable \cite{lda}. The development of several fast and accurate approximation methods applied to LDA has provided a user with several options including collapsed Gibbs sampling \cite{CGS}, collapsed variational Bayesian inference \cite{CVB}, maximum likelihood estimation \cite{ML}, and maximum a posteriori estimation \cite{MAP}. We will explore both Gibbs sampling and variational methods in the following two subsections. 

\subsubsection{Gibbs Sampling}

\noindent Gibbs sampling is a special form of the Metropolis Hasting MCMC algorithm. The idea, as applied to lda, is to approximate \ref{eq3} by repeatedly sampling from the conditional distributions of the latent variables given all other known and estimated variables at each step. It is known that the Gibbs sampler constitutes a Markov Chain whose stationary distribution is the function in question. The Markov Chain described below asymptotically converges to \ref{eq3}. Pseudo Code:
\begin{enumerate}
	\item Initialize $\beta^{(0)}, \theta^{(0)},$ and all of the topic assignments $z^{(0)}$ for each word in the vocabulary.
	\item For $t$ in $1,2,...$, sample:
	\begin{enumerate}
		\item $z^{(t)}_{i}$ from $p(z_{i}^{(t)} | z^{(t-1)}_{-i} \beta^{(t)}, \theta^{(t-1)}, w, \eta, \alpha)$
		\item $\beta^{(t)}$ from $p(\beta^{(t)} | \theta^{(t-1)}, z^{(t-1)}, w, \eta, \alpha)=Dir(\eta+c_{k,.,v})$
		\item $\theta^{(t)}$ from $p(\theta^{(t)} | \beta^{(t)}, z^{(t)}, w, \eta, \alpha)=Dir(\alpha+c_{k,d,.})$
	\end{enumerate}
	\item Continue to update and sample each of the latent variables until convergence.
\end{enumerate}


\noindent Although this algorithm will eventually converge to the distributions of the latent variables, it is slow. After noticing that the distribution of topic probabilities for each word is independent of both $\theta$ and $\beta$, a collapsed gibbs sampler can be implemented. Here, the algorithm integrates out both $\theta$ and $\beta$ from the Markov Chain's state space. Then it uses a Gibbs sampler to iteratively sample the topic assignments. After solving for converged estimates, both $\theta$ and $\beta$ can be easily estimated. For more information on the collapsed gibbs sampler, refer to \cite{CGSTheory}. After integrating out these variables, \cite{multinom} showed that for word $n$ in document $d$, the probability of word $w_{d,n}$ being assigned topic $z_{d,n}=k$ is 
\begin{equation} \label{eq4}
p( z^{t}_{d,n}=k| z^{(t-1)}_{-(d,n)}, \theta, \beta, w) =c^{-1}*\frac{(c^{-(d,n)}_{z_{d,n},d,.}+\alpha_{z_{d,n}})*(c^{-(d,n)}_{z_{d,n},.,w_{d,n}}+\beta_{w_{d,n}})}{c^{-(d,n)}_{z_{d,n},.,.}+\sum_{v=1}^{V}{\beta_{v}}} 
\end{equation}

\noindent where $c$ is the normalizing constant equal to 
$$
\sum_{k=1}^{K}{\frac{(c^{-(d,n)}_{k,d,.}+\alpha_{k})*(c^{-(d,n)}_{k,.,w_{d,n}}+\beta_{w_{d,n}})}{c^{-(d,n)}_{k,.,.}+\sum_{v=1}^{V}{\beta_{v}}}} 
$$

\noindent The update equations for $\theta$ and $\beta$ are as follows \cite{multinom}: 
\begin{equation} \label{eq5}
\hat{\theta}_{d,k}=\frac{\alpha_{k}+n_{d,k,.}}{D\alpha+n_{.,d,.}} \hspace{20mm} \hat{\beta}_{v,k}=\frac{\beta_{k,v}+n_{k,.,v}}{J\beta+n_{k,.,.}}
\end{equation}

Psuedo Code for the collapsed Gibbs sampler is as follows:
\begin{enumerate}
	\item Initialize the topic assignments for each word in the vocabulary $z^{(0)}$.
	\item For each $w_{i}$ and $t=1,2,3,...$: 
	\begin{enumerate}
		\item For each topic $k=1,2,3,...$ compute \ref{eq4}.
		\item Draw the new topic assignment from the computed discrete distribution $z^{(t)}_{i}$.
	\end{enumerate}
	\item Continue until convergence.
	\item Calculate $\hat{\theta}$ and $\hat{\beta}$ from \ref{eq5}.
\end{enumerate}

\subsubsection{Variational Bayes}
\noindent In order to infer the parameters, a second approach using variational methods was first described in \cite{lda}. This approach uses Jensen's inequality to obtain a lower bound on the log likelihood of the data given $\alpha, \eta$: $l(\alpha, \eta)=\sum_{d=1}^{D}{(\log{(w_{d} | \alpha, \eta)})}$. In general, a family of tractable surrogate distributions defined by their unique parameters is found. These parameters are referred to as variational parameters. The lower bound is found by minimizing the Kulllback-Leibler (KL) divergence between the variational distribution and the true posterior. A simple variational distribution to consider, the one that Blei considers, just removes the dependence between $\theta$ and $\beta$, which causes the intractability of the true posterior. Here we will use the following surrogate function:
$$
\tilde{q}(z, \beta, \theta)=\prod_{k}{\tilde{q}(\beta_{k} | \tilde{\eta}_{k})}\prod_{d}{\tilde{q}(\theta_{d} | \tilde{\alpha}_{d})} \prod_{d,n}{\tilde{q}(z_{d,n} | \tilde{\gamma}_{d,n})}.
$$

Minimizing the KL distance leads to the following update equations:

\begin{equation} \label{eq6}
\tilde{\gamma}^{(t+1)}_{n,d,k}=\exp{\left(\Psi{(\tilde{\alpha}^{t}_{d,k})}+\Psi{(\tilde{\eta}^{t}_{k,x_{d,n}})}-\Psi{\left(\sum_{n}{(\tilde{\eta}^{t}_{k,n})}\right)}\right)} \hspace{20mm} \tilde{\alpha}^{t}_{d,k}=\alpha+\sum_{n}{(\tilde{\gamma}^{(t+1)}_{d,k,n})} \hspace{20mm} \tilde{\eta}^{(t+1)}_{k,n}=\eta+\sum_{d,n}{(I(x_{d,n}=w)\tilde{\gamma}^{t}_{d,k,n})} 
\end{equation}

where $\Psi(x)=\frac{d\log{(\Gamma{x} ) } }{dx}$ and $I$ is the indicator function. Blei goes a step further and proposes the following EM algorithm to estimate the $\beta$ and $\alpha$ parameters:

\begin{enumerate}
	\item (E-step) Find the variational parameters by using a fixed point iterative algorithm to minimize the KL distance.
	\item (M-step) Maximize the resulting lower bound on the log likelihood with respect to $\beta$ and $\alpha$.
\end{enumerate}  

For the M-step, $\beta$ can be found analytically using 
\begin{equation} \label{eq7}
\beta^{(t+1)} \propto \sum_{d=1}^{D}\sum_{n=1}^{N_{d}}{(\tilde{\eta}_{d,k,n}w_{d,k,n})},
\end{equation} 
but $\alpha^{(t+1)}$ must be found using a Newton Raphson algorithm. Here, the estimate for $\beta$ is proportional because we need to normalize so that the $\beta$'s sum to one.

Below outlines Pseudo Code for the variational Bayes approximation algorithm:
\begin{enumerate}
	\item Initialize: \begin{align*}
						\text{For each topic } k, \hspace{10mm} &\tilde{\eta}^{0}_{n}=\eta_{n}+frac{N}{k} \\
						\text{For each document } d, \hspace{10mm}&\tilde{\alpha}^{0}_{k}=\alpha_{k}+frac{N}{k} \\
						\text{For each word in document } d, \hspace{10mm}&\tilde{\gamma}^{0}_{k}=frac{1}{k} \\
					\end{align*} 
	\item (E-step) For each document $d$ in $1,...,D$, \\ Repeat for $t=0,1,2,...$:
		\begin{enumerate}
			\item For $n$ in $1,...,N_{d}$
			\item \hspace{5mm} For $k$ in $1,...,K$ 
			\item \hspace{10mm} Update $\tilde{\gamma}^{(t+1)}_{n,d,k}$ using \ref{eq6}.
			\item \hspace{5mm} Normalize $\tilde{\gamma}^{(t+1)}_{n,d}$ to sum to one.
			\item Update $\tilde{\alpha}^{(t+1)}$ and $\tilde{\eta}^{(t+1)}$ using \ref{eq6}
			\item Continue until convergence.
		\end{enumerate}
	\item Solve the log likelihood given the data and variational estimates $\tilde{\alpha}$ and $\tilde{\eta}$.
	\item (M-step) For each document $d$ in $1,...,D$
	\item \hspace{5mm} For each $k$ in $1,..K$
	\item \hspace{10mm} For each $n$ in $1,...,N_{d}$
	\item \hspace{15mm} Solve for $\beta_{n,k}$ using \ref{eq7}
	\item \hspace{15mm} Normalize the $\beta$ so they sum to one.
	\item \hspace{5mm} Solve for $\alpha$.
	\item If likelihood converged using new parameters then end.
	\item Else go back to the E-step.
\end{enumerate}



\begin{enumerate}
	\item 
\end{enumerate}

\section{Implementation/Data Example}
\noindent In \cite{lda}, variational methods are applied to solve the problem. These methods were coded in C by Blei (refer to \cite{Ccode}) and have since been utilized in the \textit{topicmodels} package in R \cite{topicsR}. Another package, \textit{lda}, in R has been developed by Chang, which uses his own C code with collapsed Gibbs sampling for posterior approximation \cite{ldaR}. On the website Cross Validated, there is a discussion of which package is better for users, with no definitive answer \cite{STACK}. Refer to my 540 project and what I find there...

\subsection{Data}
What and how I got it...
\subsection{lda results}

\section{Conclusions}

\bibliographystyle{elsarticle-num}
\bibliography{library}

\begin{thebibliography}{4}
	\bibitem{STACK}
	Anonymous. (2012),
	``Two R Packages for Topic Modeling, lda and topicmodels?" \textit{Cross Validated}: http://stats.stackexchange.com/questions/24441/two-r-packages-for-topic-modeling-lda-and-topicmodels.
	
	\bibitem{lda}
	Blei, D., Ng, A., and Jordan, M.  (2003),
	``Latent Dirichlet Allocation."
	\textit{Journal of Machine Learning Research}: 3 993-1022.
	
	\bibitem{Ccode}
	Blei, D. (2004),``LDA-C."
	
	\bibitem{news}
	Blei, D.  (2012),
	``Survaying a Suit of Algorithms that offer a solution to managing large document archives:          Probalistic Topic Models." \textit{Communications fo the ACM}: vol.55, no.4, 77-84.
	
	\bibitem{ldaR}
	Chang, J.  (2015),
	``Package `lda'."
	\textit{CRAN}.
	
	\bibitem{MAP}
	Chien, J., and Wu, M.  (2008),
	``Adaptive Bayesian Semantic Analysis."
	\textit{Audio, Speech, and Language Processing}, IEEE Transactions on: 16(1), 198-207.
	
	\bibitem{CGS}
	Griffiths, L. and Steyvers, M.  (2004),
	``Finding Scientific Topics."
	\textit{PNAS}: 1(Suppl 1), 5228-5235.
	
	\bibitem{topicsR}
	Grun, B. and Hornik, K.  (2015),
	``Package `topicmodels'."
	\textit{CRAN}.
	
	\bibitem{ML}
	Hofmann, T.  (2001),
	``Unsupervised Learning by probabilistic Latent Semantic Analysis."
	\textit{Machine Learning}: 42(1), 177-196.
	
	\bibitem{CGSTheory}
	Liu, Jun.  (1994),
	``The Collapsed Gibbs Sampler in Bayesian Computations with Applications to a Gene Regulation Problem." \textit{ASA Jorunal}: Vol.89, No.427 Theory and Methods, 958-966.
	
	\bibitem{CVB}
	Teh, Y. W., Newman, D., and Welling, M.  (2007),
	``A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet 
	Allocation."\textit{NIPS}: 3, 1353-1360.
	
	\bibitem{multinom}
	Carpenter, Bob. (2010),
	``Integrating out Multinomial Parameters in Latent Dirichlet Allocation and Naive       Bayes for Collapsed Gibbs Sampling."
	\textit{LingPipe}: carp@lingpipe.com.
	
	
\end{thebibliography}

\end{document}


