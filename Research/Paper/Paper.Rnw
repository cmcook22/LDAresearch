\documentclass[3p,times]{elsarticle}
\usepackage{setspace}
\usepackage{algcompatible}
\usepackage[usenames,dvipsnames]{color}
\usepackage{amsthm,amsmath}
\newtheorem{definition}{Definition}
\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{hyperref}% add hypertext capabilities
\usepackage[mathlines]{lineno}
\usepackage{amssymb}
\usepackage[figuresright]{rotating}


\usepackage{Sweave}
\begin{document}
\SweaveOpts{concordance=TRUE}
\input{test-concordance}
\input{title}
\input{abstract}

\section{Introduction}
For the time being, I am studying the statistical properties of the Latent Dirichlet Allocation topic model.

\section{Background on LDA}
With an increase in the amount of digitized text, topic modeling has become an important area of research. One of the most popular methods for topic modeling is Latent Dirichlet Allocation (LDA) first described in \cite{lda}. LDA is a probabilistic hierarchical generative model, which uses the Bayesian framework to discover hidden structure, or \textit{topics},  within a text Corpus. For a more thorough background in topic modeling, refer to \cite{news}.

\subsection{Generative Process}
Given a set of $K$ topics $k=1,...K$ and a set of $D$ documents $d=1,...,D$, also referred to as a corpus, the vocabulary is the set of unique words in the Corpus say $w_{1,1},...,w_{d,n_{d}},...,w_{D,n_{D}}$, where $n_{d}$ is the number of uniques words in document $d$ and $V=\sum_{d=1}^{D}{n_{d}}$. The goal of this algorithm as stated by Blei, ``to find a probabilistic model of a corpus that not only assigns high probability to members of the corpus, but also assigns high probability to other `similar' documents" (\cite{lda}, pp.996). The generative process for a corpus is as follows:
\begin{itemize}
	\item For each topic, \\
	1. Draw $\beta^{k} \sim Dir(\eta)$, a distribution for each topic over the vocabulary.
	
	\item For each document, \\
	2. Draw $\theta^{d} \sim Dir(\alpha)$, a distribution for each document over the topics.		\begin{itemize}
		\item For each word in document $d$, \\
		3. Draw $z_{d,n} \sim Mult(\theta^{d})$, a topic assignment (i.e. $z_{d,n} \in \mathbb{Z} \in \{1,...,K\}$ ). \\
		4. Draw $w_{d,n} \sim Mult(\beta^{z_{d,n}})$, the $n$th word in document $d$.
	\end{itemize}
\end{itemize}

This process requires that we know the number of topics $K$ a priori. Blei also assumed that the hyperparameters $\eta \in  (0,1)$ and $\alpha \in (0,1)$ are fixed and can be estimated through the given data. Assuming this model, we can define the joint probability distribution of a corpus as:
\begin{equation} \label{eq1}
p(\beta,\theta,z_{d,n},w_{d,n} | \alpha,\eta) = \prod_{k=1}^{K}{p(\beta^{k} | \eta)}  \prod_{d=1}^{D}\left( p(\theta^{d} | \alpha) \prod_{n=1}^{n_{d}}{p(z_{d,n} | \theta^{d})}p(w_{d,n} | \beta^{1},...,\beta^{K},z_{d,n}) \right)  ,
\end{equation}

We will use the following dot notation:

\begin{align*}
c_{k,d,v}&=\sum_{n=1}^{n_{d}}{I(z_{d,n}=k \& w_{d,n}=v)}; &\text{ \# of times word $v$ is assigned to topic $k$ in document $d$} \\
c_{k,.,v}&=\sum_{d=1}^{D}{c_{k,d,v}}; &\text{ \# of words in document $d$ assigned to topic $k$} \\
c_{k,d,.}&=\sum_{v=1}^{V}{c_{k,d,v}}; &\text{ \# of times word $v$ is assigned to topic $k$ in any document} \\
c_{k,.,.}&=\sum_{d=1}^{D}\sum_{v=1}^{V}{c_{k,d,v}}; &\text{ \# of words in corpus assigned to topic $k$} 
\end{align*}


Let $\Gamma(\cdot)$ be the Gamma function, then:

\begin{align*}
p(\beta^{k} | \eta) &=\frac{ \Gamma( \sum_{v=1}^{V}{ \eta_{v} } )}{ \prod_{v=1}^{V}{\Gamma(\eta_{v})}}  
\prod_{v=1}^{V}{(\beta^{k}_{v})^{\eta_{v}-1}} \\
p(\theta^{d} | \alpha) &=\frac {\Gamma( \sum_{k=1}^{K}{ \alpha_{k} } )}{ \prod_{k=1}^{K}{\Gamma(\alpha_{k})}}  
\prod_{k=1}^{K}{(\theta^{d}_{k})^{\alpha_{k}-1}} \\
p(z_{d,n} | \theta) &=\frac{1}{\prod_{k=1}^{K}{c_{d,k,.}!}}
\prod_{k=1}^{K}{(\theta^{d}_{k})^{c_{d,k,.}}} \\
p(w_{d,n} | \beta,z) &=\frac{1}{\prod_{v=1}^{V}{c_{k,.,v}!}}\prod_{v=1}^{V}{
(\beta^{z_{d,n}}_{v})^{c_{k,.,v}}}
\end{align*}

\subsection{Posterior Inferance}
In practice, we do not know either $\beta$ or $\theta$. All we are given is a set of documents providing the $w_{d,n}$ parameters. Most often, we are interested in the latent variables $\beta$ and $\theta$. Rewriting \ref{eq1}

\begin{equation} \label{eq2}
p(w, z, \theta, \beta | \alpha,\eta)=\left( \prod_{k=1}^{K} \frac{\Gamma(\sum{\eta_{v}})} {\prod_{v=1}^{V}(\Gamma(\eta_{v}))} \prod_{v=1}^{V} (\beta^{k}_{v})^{(\eta_{v}-1+c_{k,.,v})} \right) * 
\left( \prod_{d=1}^{D} \frac{\Gamma(\sum{\alpha_{k}})} {\prod_{k=1}^{K}(\Gamma(\alpha_{k}))} \prod_{k=1}^{K} (\theta^{d}_{k})^{(\alpha_{k}-1+c_{k,d,.})} \right)
\end{equation}

Integrating out the latent variables, we obtain the following posterior distribution of a corpus given the hyperparameters 

\begin{equation} \label{eq3}
p(w| \theta, \beta, z)=\int_{\theta} \int_{\beta} \sum_{z} \left( \prod_{k=1}^{K}{\frac{\Gamma(\sum{\eta_{v}})}{\prod_{v=1}^{V}(\Gamma(\eta_{v}))} \prod_{v=1}^{V}{(\beta^{k}_{v})^{\eta_{v}-1+c_{k,.,v}} } }    \right) * \left( \prod_{d=1}^{D}{\frac{\Gamma(\sum{\alpha_{k}})}{\prod_{k=1}^{K}(\Gamma(\alpha_{k}))}\prod_{k=1}^{K}{
	(\theta^{d}_{k})^{\alpha_{k}-1+c_{k,d,.}} } }    \right) d\beta d\theta
\end{equation}

This distribution can be used for maximum likelihood estimation and to find the distribution of the latent variables. However, it has been proven to be intractable \cite{lda}. The development of several fast and accurate approximation methods applied to LDA has provided a user with several options including collapsed Gibbs sampling \cite{CGS}, collapsed variational Bayesian inference \cite{CVB}, maximum likelihood estimation \cite{ML}, and maximum a posteriori estimation \cite{MAP}. We will explore both Gibbs sampling and variational methods in the following two subsections. 

\subsubsection{Gibbs}

Explain Gibbs Sampling some...Note that the conditional distribution of $\beta$ is independent of all variables except for $\eta$. Its update equation at step $t$:  
\\
$$
p(\beta^{(t)} | \beta^{(t-1)},\theta, z, w, \alpha, \eta) \propto p(\beta{(t)} | 
\beta^{(t-1)}, \eta) \sim Dir(\eta+c_{k,.,v})
$$
Simerarily, we eaily find the update at step $t$ for $\theta$:
\\
$$
p(\theta^{(t)} | \theta^{(t-1)},\beta, z, w, \alpha, \eta) \propto p(\theta{(t)} | 
\theta^{(t-1)}, \alpha) \sim Dir(\alpha+c_{k,d,.})
$$
The update equation at step $t$ for the topic assignments $z$ do not follow such a simple form. Within each document $d$, the update equation for word $n$ is as follows:
\\
$$
p( z^{t}_{d,n}| z^{(t-1)}_{-(d,n)}, \theta, \beta, w, \alpha, \eta) = \frac{p(z, w | \theta, \beta)}{p(z^{t}_{-(d,n)}, w | \theta, \beta) } \propto p(z, w | \theta, \beta)=
p(z^{(t-1)}_{d,n} | \theta)p(w_{d,n} | \beta, z^{(t-1)}_{d,n})
$$

Psuedo Code...

This update although simple, cheap, and accurate, converges slowly. The collapsed Gibbs sampler was created after noticing that the algorithm could be speeded up by integrating out $\theta$ from the Markov Chain's state space. The collapsed Gibbs sampler allows us to sample on a topic-by-topic basis. After some careful calculations \cite{multinom}, one can show that for word $n$ in document $d$, the probability of word $w_{d,n}$ being assigned topic $z_{d,n}=k$ is 
\\
$$
p( z^{t}_{d,n}=k| z^{(t-1)}_{-(d,n)}, \theta, \beta, w) =c^{-1}*\frac{(c^{-(d,n)}_{z_{d,n},d,.}+\alpha_{z_{d,n}})*(c^{-(d,n)}_{z_{d,n},.,w_{d,n}}+\beta_{w_{d,n}})}{c^{-(d,n)}_{z_{d,n},.,.}+\sum_{v=1}^{V}{\beta_{v}}} 
$$
where $c$ is the normalizing constant equal to 
$$
\sum_{k=1}^{K}{\frac{(c^{-(d,n)}_{k,d,.}+\alpha_{k})*(c^{-(d,n)}_{k,.,w_{d,n}}+\beta_{w_{d,n}})}{c^{-(d,n)}_{k,.,.}+\sum_{v=1}^{V}{\beta_{v}}}} 
$$

Psuedo Code Here...
\subsubsection{Variational Methods}


\subsubsection{Other Methods Here}
Maybe...
\section{Implementation/Data Example}
In \cite{lda}, variational methods are applied to solve the problem. These methods were coded in C by Blei (refer to \cite{Ccode}) and have since been utilized in the \textit{topicmodels} package in R \cite{topicsR}. Another package, \textit{lda}, in R has been developed by Chang, which uses his own C code with collapsed Gibbs sampling for posterior approximation \cite{ldaR}. On the website Cross Validated, there is a discussion of which package is better for users, with no definitive answer \cite{STACK}. Refer to my 540 project and what I find there...

\subsection{Data}
What and how I got it...
\subsection{lda results}

\section{Conclusions}

\bibliographystyle{elsarticle-num}
\bibliography{library}

\begin{thebibliography}{4}
	\bibitem{STACK}
	Anonymous. (2012),
	``Two R Packages for Topic Modeling, lda and topicmodels?" \textit{Cross Validated}: http://stats.stackexchange.com/questions/24441/two-r-packages-for-topic-modeling-lda-and-topicmodels.
	
	\bibitem{lda}
	Blei, D., Ng, A., and Jordan, M.  (2003),
	``Latent Dirichlet Allocation."
	\textit{Journal of Machine Learning Research}: 3 993-1022.
	
	\bibitem{Ccode}
	Blei, D. (2004),``LDA-C."
	
	\bibitem{news}
	Blei, D.  (2012),
	``Survaying a Suit of Algorithms that offer a solution to managing large document archives:          Probalistic Topic Models." \textit{Communications fo the ACM}: vol.55, no.4, 77-84.
	
	\bibitem{ldaR}
	Chang, J.  (2015),
	``Package `lda'."
	\textit{CRAN}.
	
	\bibitem{MAP}
	Chien, J., and Wu, M.  (2008),
	``Adaptive Bayesian Semantic Analysis."
	\textit{Audio, Speech, and Language Processing}, IEEE Transactions on: 16(1), 198-207.
	
	\bibitem{CGS}
	Griffiths, L. and Steyvers, M.  (2004),
	``Finding Scientific Topics."
	\textit{PNAS}: 1(Suppl 1), 5228-5235.
	
	\bibitem{topicsR}
	Grun, B. and Hornik, K.  (2015),
	``Package `topicmodels'."
	\textit{CRAN}.
	
	\bibitem{ML}
	Hofmann, T.  (2001),
	``Unsupervised Learning by probabilistic Latent Semantic Analysis."
	\textit{Machine Learning}: 42(1), 177-196.
	
	\bibitem{CVB}
	Teh, Y. W., Newman, D., and Welling, M.  (2007),
	``A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet 
	Allocation."\textit{NIPS}: 3, 1353-1360.
	
	\bibitem{multinom}
	Carpenter, Bob. (2010),
	``Integrating out Multinomial Parameters in Latent Dirichlet Allocation and Naive       Bayes for Collapsed Gibbs Sampling."
	\textit{LingPipe}: carp@lingpipe.com.
	
\end{thebibliography}

\end{document}


